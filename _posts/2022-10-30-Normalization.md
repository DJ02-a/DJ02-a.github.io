---
title: "Normalization"
excerpt: ""

categories:
  - DL
  - Layer
  - Normalization
  
tags:
  - [DL]
  - [Layer]
  - [Normalization]

permalink: /DL/Normalization/

toc: true
toc_sticky: true

date: 2022-10-30
last_modified_at: 2022-10-30
---


# 1. In this articleâ€¦
---

- review and understand the most common normalization methods.
- Different methods have been introduced for different tasks and architectures.
- We will attempt to associate the tasks with the methods although some approaches are quite general.

# 2. Introduction
---

## 2.1. Importance range of features values

why we want normalization inside any model.

Imagine what will happen if the first input features are lying in different ranges.

ì–´ë–¤ input featureì˜ ê°’ ë²”ìœ„ê°€ [0, 1] ê·¸ë¦¬ê³  ë‹¤ë¥¸ input featureì˜ ê°’ ë²”ìœ„ê°€ [0,10000] ì´ë¼ë©´? ëª¨ë¸ì˜ weightê°€ ì‘ê³  ê·¼ì‚¬í•œ ë²”ìœ„ì˜ ê°’ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— [0,1] ê²½ìš°ë¥¼ ë¬´ì‹œí•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ Normalizationì´ ìƒë‹¹íˆ ì¤‘ìš”í•˜ë‹¤.

## 2.2. In model architecture

ì´ëŸ° í˜„ìƒì€ ëª¨ë¸ ì•ˆì—ì„œë„ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

ğŸ’¡ **If we think out of the box, any intermediate layer is conceptually the same as the input layer: it accepts features and transforms them.**

deep learning modelì€ ì—¬ëŸ¬ layerë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , layerì€ featuresë¥¼ inputìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì´ë‹¤. features ì—­ì‹œ ì–´ë–¤ ë²”ìœ„ì˜ ê°’ì„ ê°€ì§€ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì¡°ì ˆí•´ ì¤„ í•„ìš”ê°€ ìˆë‹¤.

<img src="/assets/images/posts_img/2022-10-30-Normalization/1.Norm-graph.png">

# 3. Kind of Normalization
---

## 3.1. Notations

- Terms
    - N : batch size
    - H : height
    - W : width
    - C : channels
    - $\mu()$ : mean
    - $\sigma()$ : standard deviation
    - y : segmentation mask
    - m : just mask

<img src="/assets/images/posts_img/2022-10-30-Normalization/2.axis.png">

$$
x,y,m \in R^{N*C*H*W}
$$

- 4D activation mapì„ 3D shapeë¡œ ì‹œê°í™”í•˜ê¸° ìœ„í•´ H, W ë‘ ì°¨ì›ì„ í•©ì¹œë‹¤.

<img src="/assets/images/posts_img/2022-10-30-Normalization/3.Norm-axis.png">

## 3.2. Batch Normalization (2015)

ğŸ’¡ **Batch Normalization (BN) normalizes the mean and standard deviationÂ for each individual feature channel/map.**

BNì€ batchë¡œ ë“¤ì–´ì˜¨ feature mapì˜ ê° channelê°„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ normalizeí•´ì¤€ë‹¤.

ì´ë¯¸ì§€ íŠ¹ì„±ìœ¼ë¡œì„œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ëŠ” first-order statisticsë¼ í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ì´ê²ƒë“¤ì€ image styleì²˜ëŸ¼ imageê°€ ê°€ì§€ëŠ” **global characteristics**ì™€ ê´€ë ¨ì´ ìˆë‹¤.

ì´ BN ë°©ë²•ì€ feature map channel ê°„ì— characteristicsë¥¼ ê³µìœ í•˜ë„ë¡ í•˜ê¸° ìœ„í•œ ì „ëµìœ¼ë¡œ ë§ì´ ì„ íƒëœë‹¤.(êµ¬ì²´ì ìœ¼ë¡œ feature map channel ê°„ ê³µí†µëœ íŠ¹ì„±ì„ ì˜ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.) ì´ëŸ¬í•œ ì´ìœ ë¡œ BNì´ downstream taskì— ë§ì´ ì‚¬ìš©ëœë‹¤.(i.e. image classification)

> Feature mapì˜ channelì—ëŠ” conv weightì™€ ê³„ì‚°í•œ ê²°ê³¼ê°€ ìˆë‹¤. Batch sizeë§Œí¼ ë“¤ì–´ì˜¨ inputë“¤ì˜ ê° feature mapì—ì„œ ë™ì¼í•œ ìœ„ì¹˜ì˜ channelë“¤ì€ ê°™ì€ conv weightì™€ ê³„ì‚°í•œ ê²°ê³¼ë“¤ì´ ìœ„ì¹˜í•  ê²ƒì´ë‹¤. BNì€ channelë³„ë¡œ meanê³¼ standard deviationì„ ê³„ì‚°í•œë‹¤ê³  í•˜ì˜€ìœ¼ë¯€ë¡œ, batch size imageì˜ feature map channelê°„ì— ë¶„í¬ë¥¼ ê³µìœ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.
> 

ìˆ˜í•™ì  ê´€ì ìœ¼ë¡œ ë³´ë©´, **Normalizationì€ ìš”ì†Œë“¤ì„ íŠ¹ì • ë²”ìœ„ ë‚´ë¡œ í•œì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, feature mapì´ BNì„ ê±°ì¹˜ê²Œ ë˜ë©´ channelë³„ë¡œ ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.** 

<img src="/assets/images/posts_img/2022-10-30-Normalization/4.BN.png">

$$
BN(x) = \gamma(\frac{x - \mu(x)}{\sigma(x)}) + \beta
$$

ì´ë ‡ê²Œ feature valuesë¥¼ Gaussian-like spaceë¡œ ë§Œë“¤ì–´ì„œ ëª¨ë¸ì„ ì˜ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

ìœ„ BNì˜ ìˆ˜í•™ ì‹ì—ì„œ $\gamma$ì™€ $\beta$ëŠ” **trainable parameters** ë¡œì¨ linear/affine transformation ì´ ë˜ë„ë¡ í•œë‹¤. ì´ ê°’ë“¤ì€ channelë³„ë¡œ ë‹¤ë¥´ë©°, ê°œìˆ˜ëŠ” channelì˜ ìˆ˜ì™€ ë™ì¼í•˜ë‹¤.

### In Pytorch

```python
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
```

- ì—¬ê¸°ì„œ affine optionì´ $\gamma$ì™€ $\beta$ë¥¼ ì‚¬ìš©í• ì§€ ë§ì§€ ì„ íƒí•˜ëŠ” ê²ƒì´ë‹¤.

### Advantages of BN

- BNì€ deep neural networkì˜ í•™ìŠµì„ ê°€ì†í™” ì‹œí‚¨ë‹¤.
- ë§¤ mini-batchë§ˆë‹¤ ë‹¤ë¥¸ ê°’ì„ ê³„ì‚°í•˜ê³  regularization í•œë‹¤. Regularizationì€ ë³µì¡í•œ deep neural networkì˜ í•™ìŠµì„ ì™„í™”ì‹œì¼œì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
- ë§¤ mini-batchë§ˆë‹¤ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ë¥¼ **mini-distributions Internal Covariate Shift**ë¼ í•œë‹¤. BNì€ ì´ í˜„ìƒì„ í•´ê²°í•´ ì¤€ë‹¤.
- BNì€ networkë¥¼ í†µê³¼í•˜ëŠ” gradient flowì—ë„ ì´ì ì„ ì¤€ë‹¤. gradientì˜ parametersì˜ initial valuesì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ì¤„ì—¬ì¤€ë‹¤. ì´ëŠ” ë†’ì€ learning rateë¥¼ ì„¤ì • ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
- nonlinearities network

### Disadvantages of BN

- ë§¤ìš° ì‘ì€ batch sizeë¡œ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ê¸°ì—” ì í•©í•˜ì§€ ì•Šë‹¤.(i.e. video prediction, segmentation and 3D medical image processing) ì´ëŠ” modelì˜ errorì„ ì¼ìœ¼í‚¨ë‹¤.
- Problems when batch size is varying. Example showcases are training VS inference, pretraining VS fine tuning, backbone architecture VS head.

### In test

í•™ìŠµë•Œ ì–»ì€ Batch size ë§Œí¼ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•œë‹¤.

â†’ feature mapì˜ ê°’ì— ë”°ë¼ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì§€ì •ëœ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•œë‹¤.

> ê° batch ë³„ feature mapì˜ í‰ê· ê³¼ ë¶„ì‚°ê°’ì´ ë¬´ì‹œë  ìˆ˜ ìˆì–´ë³´ì¸ë‹¤.
> 


## 3.3. Synchronized Batch Normalization (2018)

Training Scaleì´ ì»¤ì§ˆìˆ˜ë¡ BNì€ í•„ìˆ˜ì ì´ë‹¤. BNì´ ë‚˜ì˜¨ ì´í›„ ì—…ê·¸ë ˆì´ë“œ ë²„ì „ìœ¼ë¡œ Synchronized BN(**Synch BN**) ì´ ë‚˜ì˜¤ê²Œ ë˜ì—ˆë‹¤. **Synchronized**ì˜ ì˜ë¯¸ëŠ” ê° GPUì—ì„œ ë³„ë„ë¡œ meanê³¼ varianceë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.(ê°™ì€ ê°’ìœ¼ë¡œ í†µì¼í•´ì„œ ì—…ë°ì´íŠ¸ í•˜ëŠ”ë“¯)

ğŸ’¡ **Instead, in multi-worker setups, Synch BN indicates that the mean and standard-deviaton are communicated across workers (GPUs, TPUs etc).**

<img src="/assets/images/posts_img/2022-10-30-Normalization/5.AdaIN.png">

## 3.4. Layer Normalization (2016)

BNì€ batchì™€ spatial dims ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ì—ˆë‹¤. ë°˜ëŒ€ë¡œ **Layer Normalization(LN)**ì€ ëª¨ë“  channelsê³¼ spatial dimsì— ëŒ€í•´ì„œ Normalization í•œë‹¤. ê·¸ë˜ì„œ LNì€ batchì™€ ë…ë¦½ì ì¸ ê´€ê³„ë¥¼ ê°€ì§„ë‹¤. ì´ layerëŠ” handle vectorsë¡œ ì²˜ìŒì— ì •í•´ì§„ë‹¤.(mostly the RNN outputs.)

Transformers ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ëŠ” ì£¼ëª©ë°›ì§€ ëª»í•œ Normì´ë‹¤.

> LNì€ batchì™€ ë…ë¦½ì ì¸ Normalization ì´ë¯€ë¡œ ì ì€ batch sizeì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì‚¬ì‹¤ìƒ ì¤‘ê°„ layerì˜ output feature mapì„ í†µì§¸ë¡œ Normí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— Layer Normalizationì´ë¼ ì´ë¦„ì„ ë¶™ì¸ ê²ƒ ê°™ë‹¤.

computation of mean and std in Synch BN

$$
\sigma^{2} = \frac{\sum^{N}_{i=1}(x_{i} - \mu)^{2}}{N} = \frac{\sum^{N}_{i=1}x^{2}_{i}}{N} - \frac{(\sum^{N}_{i=1})^{2}}{N^{2}}
$$

## 3.5. Instance Normalization : The Missing Ingredient for Fast Stylication (2016)

ğŸ’¡ **Instance Normalization (IN) is computedÂ â€œonly across the featuresâ€™ spatial dimensionsâ€. So it is independent for each channel and sample.**

BNê³¼ INì„ ë¹„êµí•˜ìë©´ BNì—ì„œ *N dimension*ì„ ì œì™¸í•˜ê³  Normalizationí•œ ê²ƒì´ INì´ë‹¤. ì˜¤ë¡œì§€ spatial dimension ì°¨ì›ì— ëŒ€í•´ì„œ normalization í•˜ëŠ” ë°©ì‹ì´ë‹¤.  INì€ each individual sampleì˜ style ì •ë³´ë¥¼ meanê³¼ standard deviationì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ ì´ìš©í•˜ë©´ denormalizationìœ¼ë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ë°”ê¿”ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.(modeled by $\gamma,\space \beta$)

ì´ë¡œì¨ ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ë½‘ê¸°ë„ í•˜ê³  ì „ë‹¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. style ì •ë³´ë¥¼ ë°›ì€ ëª¨ë¸ì€ style informationì„ í•™ìŠµí•˜ëŠ”ë° ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ì§€ ì•Šì•„ë„ ë˜ì—ˆê³  content manipulation, local detail ê°™ì´ ë‹¤ë¥¸ ë¶€ë¶„ì„ í•™ìŠµí•˜ëŠ”ë° focusingì´ ê°€ëŠ¥í•´ì ¸ í•™ìŠµì„ ìˆ˜ì›”í•˜ê²Œ í•œë‹¤.

$$
IN(x) = \gamma\frac{x-\mu(x)}{\sigma(x)} + \beta
$$

## 3.6. Adaptive Instance Normalization (2017)

Normalizationê³¼ style transferëŠ” ê´€ë ¨ë„ê°€ ë†’ë‹¤. INì—ì„œ $\gamma, \space\beta$ëŠ” ìŠ¤íƒ€ì¼ ê´€ë ¨ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— image $y$ ì— $\gamma, \space\beta$ ë¡œ denormalizationì„ í•´ì¤€ë‹¤ë©´ image yëŠ” image xì˜ styleì„ ê°€ì§€ê²Œ ëœë‹¤.

ğŸ’¡ **Adaptive Instance Normalization (AdaIN) receives an input imageÂ x*x*Â (content) and a style inputÂ y*y*, and simply aligns the channel-wise mean and variance of x to match those of y. Mathematically:**


$$
AdaIN(x,y) = \sigma(y)\frac{x - \mu(x)}{\sigma(x)} + \mu(y)
$$

> feature mapì˜ channelë³„ meanê³¼ standard deviationì€ input imageì˜ styleê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
input imageì˜ structure ì •ë³´ëŠ” ì–´ë–»ê²Œ ì°¾ì„ ìˆ˜ ìˆì„ì§€??

## 3.7. Group Normalization (2018)

GNì€ deeplearning ì´ì „ object detection taskë¥¼ ìœ„í•´ ì´ìš©ëœ HOG feature ë°©ì‹ì„ ë”°ë¥¸ë‹¤.

**HOG feature ë°©ì‹ ìˆœì„œ**
1. imageì•ˆì— ì—¬ëŸ¬ patch ì˜ì—­ì„ ì •í•œë‹¤.
2. ê° patch ì˜ì—­ë³„ë¡œ histogramì„ ê³„ì‚°í•˜ê³  Normalizationì„ ì§„í–‰í•œë‹¤.
3. concatenate í•˜ì—¬ ìµœì¢… feature mapì„ ì–»ëŠ”ë‹¤.

GNì—­ì‹œ channelì„ Nê°œì˜ groupìœ¼ë¡œ ë‚˜ëˆ„ê³  group ë³„ë¡œ Normalization í•˜ì—¬ feature mapì„ ì–»ëŠ”ë‹¤.

ğŸ’¡ **Group normalization (GN) â€œdividesâ€Â the channels into groups and computes the first-order statistics within each group.**

GNì€ **í° batch sizeë¡œ** í•™ìŠµí•˜ëŠ” BNë³´ë‹¤ ì•ˆì •ë˜ê²Œ í•™ìŠµë˜ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. 

ğŸ’¡ **For groups=number of channels we get instance normalization, while for`groups=1 the method is reduced to layer normalization.**

$$
\mu_{i} = \frac{1}{m}\sum_{k\in S_{i}}, \space \sigma_{i} = \sqrt{\frac{1}{m}\sum_{k\in S_{i}}(x_{k} - \mu{i})^{2}+\epsilon}
$$

$$
S_{i} = \left\{ {k|k_{N}}, \left [ \frac{kC}{\frac{C}{G}}\right ] = \left [ \frac{iC}{\frac{C}{G}} \right ] \right \}
$$

- G : the number of groups(hyper parameter)
- C/G : the number of channels per group
- GNì€ groupìˆ˜ ë§Œí¼ meanê³¼ standard deviationì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤.

> GNì´ object detection taskë¥¼ ìœ„í•´ì„œ ê³ ì•ˆëœ Normalization ì¢…ë¥˜ë¼ ìƒê°ëœë‹¤.


## 3.8. Spectral Normalization for Generative Adversarial Networks (2018)

GAN í•™ìŠµì—ì„œ ë¶ˆì•ˆì •ì„±ì´ ê³„ì†í•´ì„œ ì–¸ê¸‰ë˜ì–´ ì™”ìŒ. ì´ì— discriminatorì˜ í›ˆë ¨ì„ ì•ˆì •í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” spectral normalizationì„ ìƒˆë¡­ê²Œ ì œì•ˆí•˜ì˜€ìŒ. 

- ê³„ì‚°ëŸ‰ì„ ì¤„ì—¬ì¤Œ
- ì‰½ê²Œ ì ìš© ê°€ëŠ¥
- í•™ìŠµì„ ì•ˆì •ì‹œí‚´

ì´ì „ì—ë„ weightë¥¼ normalizationí•˜ëŠ” ë°©ë²•ì´ ìˆì—ˆë‹¤. weight normalization, weight clipping(WGAN), gradient penalty(WGAN-GP)

# 4. Appendix
---

## 4.1. GN

### GN vs else Normalizaiton

<img src="/assets/images/posts_img/2022-10-30-Normalization/6.GN.png">
<img src="/assets/images/posts_img/2022-10-30-Normalization/7.compare.png">

- BNì‚¬ìš© ì‹œ Batch sizeëŠ” 16ì´ìƒì€ ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì„
- GNì€ ì‘ì€ Batch sizeì—ë„ ì ì€ errorì„ ë³´ì„
    - ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥
- ë™ì¼í•œ Batch sizeë¡œ ê°ê° í•™ìŠµí•˜ë©´ GNì´ ë” í•™ìŠµ ì˜ë¨

### Amount of groups

<img src="/assets/images/posts_img/2022-10-30-Normalization/8.val.png">

- ì‹¤í—˜ ìƒ 8, 32ê°œê°€ ê°€ì¥ ì í•©.
- batch sizeë¥¼ 32ê°œ ì´ìƒ í•´ë„ ë  ë“¯?

## 4.2. Norm layer in model architecture

## Q1

[https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/782](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/782)

### Q1.1 Pix2Pixì—ì„œ first down-sampling blockì— INì„ ì ìš©í•˜ì§€ ì•ŠìŒ. ì™œ?

INì˜ íŠ¹ì„±ì€ inputì˜ styleì´ë‚˜ color ì •ë³´ë¥¼ ì‚­ì œí•˜ëŠ” ê²½í–¥ì„ ì§€ë‹Œë‹¤. ì €ìëŠ” ì´ style ì •ë³´ë¥¼ ì¡°ê¸ˆ ë” ë³´ì¡´í•˜ê³  ì‹¶ì–´ ì²« blockì— INì„ ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•œë‹¤.

### Q1.2. ê·¸ë ‡ë‹¤ë©´ BNì„ first blockì— ì ìš©í•˜ë©´ ì•ˆë˜ë‚˜?

BNì€ individual imageë³´ë‹¤ ì „ì²´ datasetì— ëŒ€í•´ í†µê³„ë¥¼ ê³„ì‚°í•´ ì¤€ë‹¤. ê·¸ë¦¬ê³  BNìœ¼ë¡œ ê° ì´ë¯¸ì§€ì˜ style ì •ë³´ë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆì„ ë“¯.

> ê°œì¸ì ì¸ ìƒê°ìœ¼ë¡œ individual imageì˜ styleì„ encoderë¥¼ í†µí•´ ë½‘ì•„ë‚´ê³  ì‹¶ë‹¤ë©´ INì´ ì í•©í•´ ë³´ì¸ë‹¤.
INì„ í†µí•´ì„œ norm ë˜ëŠ” ì •ë³´ëŠ” colorì¸ì§€ styleì¸ì§€ ë‘˜ ë‹¤ ì¸ì§€ ì˜ë¬¸

## Q2

[https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/981](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/981)

### Q2.1. ì™œ CycleGAN êµ¬ì¡°ì—ì„œ first blockì—ì„œ INì„ ì ìš©í–ˆëŠ”ê°€?

ì•„ê¹Œ Q1.1. ê³¼ ë°˜ëŒ€ë˜ëŠ” ê²½ìš°ë‹¤. CycleGANì€ resnet-based architectureë¥¼ ë”°ë¥¸ ê²ƒì´ë‹¤. architectureë¥¼ ë³´ë©´ first blockì— í° Conv layer(7x7)ê°€ norm layer ì „ì— ì¡´ì¬í•œë‹¤. ì´ëŸ¬ë©´ color informationì´ ì¶©ë¶„íˆ encode ëœ ê²ƒì´ë¼ ë³´ê¸° ë•Œë¬¸ì— INì„ ë„£ì€ ê²ƒì´ë‹¤.

### Q2.2. use_bias = norm_layer == nn.InstanceNorm2d ì˜ ì˜ë¯¸ëŠ”?

flag `affine` ì— ë‹¬ë ¤ìˆëŠ” ì˜µì…˜ì´ë‹¤. IN layerì˜ optionì´ `affine=True` ë¼ë©´ Conv layerì—ì„œ `bias=False` ë¥¼ ì„¤ì •í•´ ì¤˜ì•¼ í•œë‹¤.

ì–´ì°¨í•€ Normì˜ affineì— ì˜í•´ì„œ biasëŠ” ì˜ë¯¸ê°€ ì—†ì–´ì ¸ ë²„ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤.

**Reference**

[Batch Normalization(BN)](https://kjhov195.github.io/2020-01-09-batch_normalization/)
    
[PyTorchì—ì„œ ë‹¤ì¤‘ GPU ë™ê¸°í™” ë°°ì¹˜ ì •ê·œí™” êµ¬í˜„ - wenyanet](https://www.wenyanet.com/opensource/ko/603fe7a794216c52da4597e4.html)

[In-layer normalization techniques for training very deep neural networks AI Summer](https://theaisummer.com/normalization/)

[instance vs BN](https://www.baeldung.com/cs/instance-vs-batch-normalization)

[ë…¼ë¬¸ Summary SNGAN (2018 ICLR) "Spectral Normalization for Generative Adversarial Networks"](https://aigong.tistory.com/371#%EB%85%BC%EB%AC%B8_%EB%A7%81%ED%81%AC)

[Group Normalization](https://youtu.be/m3TN9FFmqsI)

[Conv2d - PyTorch 1.12 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)

[InstanceNorm2d - PyTorch 1.12 documentation](https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html)
