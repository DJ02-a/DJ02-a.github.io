---
title: "VAE(Variational Auto-Encoder)"
excerpt: ""

categories:
  - DL
tags:
  - [VAE, DL]

permalink: /DL/VAE/

toc: true
toc_sticky: true

date: 2022-10-22
last_modified_at: 2022-10-22
---

# 1. Key Point

---

1. VAE는 Generative Model 이다.
    - Generative model : training data가 주어졌을 때 training data가 가지는 real 분포와 같은 분포에서 sampling된 값으로 new data를 생성하는 model
        
        > 필요한 것은 sampling할 real data의 분포
        어떻게 real data의 분포를 얻지?
        > 
2. 확률 통계 이론(Bayseain, conditional prob)
3. terms
    - latent : ‘잠재하는', ‘숨어있는', ‘hidden’의 뜻을 가진 단어. 여기서 말하는 latent variable z는 특징(feature)을 가진 vector로 이해하면 좋다.
    - interactable : 문제를 해결하기 위해 필요한 시간이 문제의 크기에 따라 지수적으로(exponential) 증가한다면 그 문제는 난해(interactable)하다고 한다.
    - explicit density model : 샘플링 모델의 구조(분포)를 명확히 정의
    - implicit density model : 샘플링 모델의 구조(분포)를 explicit하게 정의하지 않음
    - density estimation : x라는 데이터만 관찰할 수 있을 때, 관찰할 수 없는 x가 샘플된 확률밀도함수를(probability density function)을 estimate 하는 것
    - Gaussian distribution : 정규 분포
    - Bernoulli distribution : 베르누이 분포
        - Reference
            
            [[이산형 분포] 베르누이 분포(Bernoulli distribution), 이항 분포(Binomial distribution)](https://soohee410.github.io/discrete_dist1)
            
    - Marginal Probability : 주변 확률 분포
    - $D_{kl}$ : Kullback-Leibler divergence(KDL), 두 확률분포의 차이
    - likelihood : 가능도
        - Reference
            
            [RPubs](https://rpubs.com/Statdoc/204928)
            
        

# 2. VAE

---

## 2.1. Goal

[ In papers… ]
“*How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets?”*

> 확률 모델을 효과적으로 추론하는 방법?
large dataset을 continuous latent variable로 바라보고 그 분포를 효과적으로 학습하는 model을 얻는 방법에 대해서 알아본다.
> 

<img src="/assets/images/posts_img/2022-10-22-VAE/1.vae.png">


VAE의 목표는 Generative Model 의 목표와 같다.

(1) data와 같은 분포를 가지는 sample 분포에서 sample을 뽑고

training data가 어떤 p_data(x)(확률 밀도 함수) 분포를 가지고 있다면, sample 모델 p_model(x) 도 같은 분포를 가져야 한다.

(2) 어떤 새로운 것을 생성해내는 것

모델을 통해 나온 inference 값이 training data에 없는 새로운 x’ 라는 데이터이길 바람.

> generative model이 trainig data가 가진 분포와 동일하게 만드는 것이 목표로 보인다.
분포를 비슷하게 만들어서 training data에 존재하지 않지만 비슷한 결과물을 만드는 것도 주요 목표이다.
> 

## 2.2 VAE 구조

<img src="/assets/images/posts_img/2022-10-22-VAE/2.flow.png">


### 1) Encoder

- input : x
- output : $\mu_{i}, \sigma_{i}$

x를 통해 데이터의 특징(latent variable)을 추측한다. 구체적으로는 latent variable의 분포를 나타내는 $\mu$와 $\sigma$를 예측한다.(기본적으로 여기서 나온 특징들의 분포는 정규분포를 따른다고 가정한다. )

> training dataset 또한 어떤 차원 상에서 분포를 가지고 있다고 볼 수 있겠다. 이를 Encoder를 통해 작은 차원으로 줄인다. 여기서 Encoder는 단순히 작은 차원으로 줄이는 것 뿐 아니라 작은 차원에서의 분포를 학습하는 역할 또한 가지고 있는 것처럼 보인다.
> 

 

### 2. Reparameterization Trick(Sampling)

VAE가 하고 싶은 것은 어떤 data의 true 분포가 있으면 그 분포에서 하나를 뽑아 기존 DB에 있지 않는 새로운 data를 생성하는 것이다. 그래서 데이터의 확률분포를 알아야 하고, 그 확률분포에서 sampling을 해야한다.

<img src="/assets/images/posts_img/2022-10-22-VAE/3.trick.png">


> Encoder에서 나온 분산과 평균값을 가지고 바로 sampling하게 된다면, encoder와 decoder의 연결점이 사라진다.
차라리 정규분포에서 sampling한 값에 분산과 평균값을 적용한다면 학습 가능한 node가 생성된다. 그리고  이 node는 연결점이 될 수 있다.
> 

$N(\mu_{i},\sigma_{i}^{2}1)$ 에서 sampling 하는 것이랑 정규분포 $N(0,1)$에서 sampling한 숫자에 분산을 곱하고 평균을 더한 분포는 동일하다. 하지만 후자를 선택하게 되면 학습이 가능해 진다. 이를 reparameterization trick 이라 한다.

이런 trick을 통해 z를 만들 수 있다.

### 3. Decoder

z값을 decoder에 넣고 원래 이미지 사이즈의 아웃풋이 나오게 된다. 이때 p_data(x)의 분포를 **Bernoulli** 로 가정했으므로 output 값은 0~1 사이 값을 가져야 하고, 이를 위해서 마지막 activation function을 sigmoid로 설정해 준다.

> 각 픽셀 값의 분포를 0 or 1 값을 가지는 bernoulli distribution은 mnist dataset에서는 적합한 가정이라 생각한다. 
color image에서도 적합한지 궁금하다. input이 bernoulli distribution을 따른다는 가정은 적합하지 않을듯 하다. 0~255 범위의 값들을 0~1로 normalization 해도 안될듯.
> 
- Reference
    
    [[논문] VAE(Auto-Encoding Variational Bayes) 직관적 이해](https://taeu.github.io/paper/deeplearning-paper-vae/)
    

# 3. Train VAE

---

## 3.1. Loss Function

<img src="/assets/images/posts_img/2022-10-22-VAE/4.loss.png">


Loss = Reconstruction Error + Regularization

### Reconstruction Error

input image x와 만들어진 output과의 관계를 살펴봐야 한다. 둘 간의 cross entropy로 loss를 구한다.

### Regularization

x가 가지는 분포와 동일한 분포를 가지도록 하기위해 true 분포를 approximate 한 함수의 분포에 대한 loss term

이 때 loss는 true pdf와 approximated pdf간의 D_kl을 계산한다.

> Reconstruction을 위해서는 L1, L2 loss도 있을텐데 Bernoulli distribution을 따른다고 해서 cross entropy를 사용할 필요가 있는지 의문.
> 

# 4. Result of VAE

---

<img src="/assets/images/posts_img/2022-10-22-VAE/5.result.png">


- Code
    
    ```python
    import matplotlib.pyplot as plt
    from scipy.stats import norm
    
    n=20
    digit_size = 28
    figure = np.zeros((digit_size*n,digit_size*n))
    grid_x = norm.ppf(np.linspace(0.05,0.95,n))
    grid_y = norm.ppf(np.linspace(0.05,0.95,n))
    # 결과물을 뽑는 코드
    # z vector 2차원으로 설정
    # 각 축마다 일정하게 다른 값을 설정하여 어떤 결과물이 나오는지 체크
    for i, yi in enumerate(grid_x):
        for j, xi in enumerate(grid_y):
            z_sample = np.array([[xi,yi]])
            z_sample = np.tile(z_sample,batch_size).reshape(batch_size,2)
            x_decoded = decoder.predict(z_sample, batch_size = batch_size)
            digit = x_decoded[0].reshape(digit_size, digit_size)
            figure[i * digit_size: (i+1)*digit_size, j*digit_size:(j+1)*digit_size] = digit
            
    plt.figure(figsize=(10,10))
    plt.imshow(figure, cmap ='Greys_r')
    plt.show()
    ```
    

위의 코드를 실행시키면 위 그림에서 오른쪽과 같은 도식이 나오는데 학습이 잘 되었다면 차원의 manifold를 잘 학습했다는 말이다. 그 manifold를 2차원으로 축소시킨 것(z1,z2)에서 z1 20개(0.05~0.95), z2 20개, 총 400개의 순서쌍의 xi,yi에서 sample을 뽑아 시각화한것이 오른쪽 그림인데 2D상에서 거리의 유의미한 차이에 따라 숫자들이 달라지는 것을 확인할 수 있으며, 각 숫자 상에서도 서로 다른 rotation들을 가지고 있다는 것이 보인다.
