---
title: "Train Skill"
excerpt: ""

categories:
  - DL
  - Data Augmentation
  - BN
  - Scheduler
  - MultiGPU
  - Accumulation
  
tags:
  - [DL]
  - [Data Augmentation]
  - [BN]
  - [Scheduler]
  - [MultiGPU]
  - [Accumulation]

permalink: /DL/Train-Skill/

toc: true
toc_sticky: true

date: 2022-11-12
last_modified_at: 2022-11-12
---



# 1. Data
---

## 1.1. Data Augmentation

í›ˆë ¨ ë°ì´í„°ë¥¼ ì„ì˜ì ìœ¼ë¡œ ìˆ˜ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•. ì˜¤ë²„í”¼íŒ… ë¬¸ì œ í•´ê²°ê³¼ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤.

- Options
    
    ì•ì— í•­ìƒ `transforms.` ê°€ ë¶™ëŠ”ë‹¤.
    
    - `CenterCrop(size)` : ì¤‘ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ size ë§Œí¼ ìë¥¸ë‹¤.
    - `ColorJitter(0,0,0,0)` : ì´ë¯¸ì§€ì˜ brightness, contrast, saturation, hue ì •ë„ë¥¼ ì¡°ì ˆí•œë‹¤.
    - `Grayscle()` : color imageë¥¼ gray imageë¡œ ë°”ê¿”ì¤€ë‹¤. ì±„ë„ì€ 1ì´ëœë‹¤.
    - `Pad(,fill,padding_mode)` : image ê°€ì¥ìë¦¬ë¥¼ paddingí•œë‹¤.
        - fill(default = 0)
        - padding_mode : padding ë°©ì‹ ì„¤ì •(edge, reflect, symmetric)
    - `RandomCrop(size)` : ëœë¤í•œ ìœ„ì¹˜ì—ì„œ ì´ë¯¸ì§€ë¥¼ sizeë§Œí¼ cropí•œë‹¤.
    - `RandomHorizontalFlip(p)` : pí™•ë¥ ë¡œ ì´ë¯¸ì§€ë¥¼ ë’¤ì§‘ëŠ”ë‹¤.
    - `Resize(size)` : ì´ë¯¸ì§€ í¬ê¸° ë³€í™˜
    - `GaussianBlur(kernel_size)` : ì´ë¯¸ì§€ì— blur ì ìš©
    
    dtypeì´ Tensorì¼ë•Œë§Œ ì‚¬ìš©ê°€ëŠ¥í•œ options
    
    - `Normalize()`
    - `RandomErasing()` : ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¥¼ ì§€ì›Œì¤€ë‹¤.
    - `ToPILImage()` : tensor to PIL image

```python
import torchvision.transforms as transforms

# def transforms
transforms = transforms.Compose(
	transforms.Resize((512,512)),
	transforms.ToTensor(),
	...
)
```

- Reference
    
    [torchvision.transforms - Torchvision master documentation](https://pytorch.org/vision/0.9/transforms.html)
    

## 1.2. Data Normalization

ì…ë ¥ë˜ëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œ ê³µê°„ìƒ ë¶„í¬ë¥¼ ì •ê·œí™”ì‹œì¼œì£¼ë©´ ë” ë†’ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì´ìš©í•œë‹¤.

ì…ë ¥ ë°ì´í„°ë¥¼ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì¸ ì •ê·œë¶„í¬ê°€ ë˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì„ í‘œì¤€í™”(Standardization)ë¼ í•œë‹¤.

- transforms.Normalize()ëŠ” ë°˜ë“œì‹œ transforms.ToTensor() ë’¤ì— ìœ„ì¹˜í•´ì•¼ í•œë‹¤.

```python
import torchvision.transforms as transforms

# def transforms
transforms = transforms.Compose(
	transforms.ToTensor()
	transforms.Normalize((í‰ê· ),(í‘œì¤€í¸ì°¨))
)
```

ì‚¬ì‹¤ ì´ë¯¸ì§€ì˜ ê²½ìš° í”½ì…€ ê°’ë“¤ì´ ì „ë¶€ 0 ~ 255 ë²”ìœ„ë¡œ ì œí•œë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë°˜ë“œì‹œ Normalizeë¥¼ í•  í•„ìš”ëŠ” ì—†ë‹¤ê³ ë„ í•œë‹¤.

- Options
    - transforms.ToTensor()
        - input dataë¥¼ 0~1 ë²”ìœ„ë¡œ scaleë¥¼ ë°”ê¿”ì¤€ë‹¤.
    - transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
        - 0~1 ë²”ìœ„ tensorë¥¼ -1~1 ë²”ìœ„ë¡œ Normalize í•œë‹¤.
        - ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
    - transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))
        - ImageNet ë°ì´í„°ì…‹ í•™ìŠµì‹œ ì–»ì–´ë‚¸ ê°’ë“¤(ImageNetì„ ì‚¬ìš©í• ë•Œ í•„ìš”í•  ë“¯)
        - ImageNet ë°ì´í„°ì…‹ì— ê³ í’ˆì§ˆì˜ ë°ì´í„°ê°€ ë§ì´ ì¡´ì¬í•˜ë¯€ë¡œ ì´ë¥¼ ë”°ë¥´ë©´ í•™ìŠµì´ ì˜ ë ê²ƒì´ë¼ íŒë‹¨
- Reference
    
    [ë”¥ëŸ¬ë‹ í•™ìŠµ í–¥ìƒì„ ìœ„í•œ ê³ ë ¤ ì‚¬í•­ë“¤](http://www.gisdeveloper.co.kr/?p=8443)
    
    [Pytorch torchvision.transforms.normalize í•¨ìˆ˜](https://guru.tistory.com/72)
    
    [[Pytorch] ì´ë¯¸ì§€ ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•œ í‰ê· (mean)ê³¼ í‘œì¤€í¸ì°¨(std) êµ¬í•˜ê¸°](https://eehoeskrap.tistory.com/463)
    

# 2. Weight initialization

---

ì‹ ê²½ë§ì´ ê¹Šì–´ì§ˆ ìˆ˜ë¡ ê° ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ ê°’ë“¤ì˜ ë¶„í¬ê°€ í•œìª½ìœ¼ë¡œ ì ë¦´ ìˆ˜ ìˆë‹¤. ì´ëŸ° í˜„ìƒì€ Gradient Vanishingì´ ë°œìƒí•  ìˆ˜ ìˆê³ , ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥ì— ì œí•œì´ ë°œìƒí•œë‹¤.

ì´ë¥¼ ìœ„í•´ì„œ í•™ìŠµí•˜ê¸° ì „ì— ê°€ì¤‘ì¹˜ë¥¼ ì ë‹¹í•˜ê²Œ ì´ˆê¸°í™” í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ë°©ë²•ì€ **Xavier**, **He** ë“± ì´ ì¡´ì¬í•œë‹¤.

- activation functionì„ ReLU, leaky_ReLUë¡œ ì‚¬ìš©í•œë‹¤ë©´ He ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.
- Code
    
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.init as init
    
    class CNN(nn.Module):
        def __init__(self):
            super(CNN,self).__init__()
            self.layer = nn.Sequential(
                nn.Conv2d(1,16,3,padding=1),  # 28 x 28
                nn.ReLU(),
                nn.Conv2d(16,32,3,padding=1), # 28 x 28
                nn.ReLU(),
                nn.MaxPool2d(2,2),            # 14 x 14
                nn.Conv2d(32,64,3,padding=1), # 14 x 14
                nn.ReLU(),
                nn.MaxPool2d(2,2)             #  7 x 7
            )
            self.fc_layer = nn.Sequential(
                nn.Linear(64*7*7,100),
                nn.ReLU(),
                nn.Linear(100,10)
            )
    
            # ì´ˆê¸°í™” í•˜ëŠ” ë°©ë²•
            # ëª¨ë¸ì˜ ëª¨ë“ˆì„ ì°¨ë¡€ëŒ€ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            for m in self.modules():
                # ë§Œì•½ ê·¸ ëª¨ë“ˆì´ nn.Conv2dì¸ ê²½ìš°
                if isinstance(m, nn.Conv2d):
                    '''
                    # ì‘ì€ ìˆ«ìë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•
                    # ê°€ì¤‘ì¹˜ë¥¼ í‰ê·  0, í¸ì°¨ 0.02ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    m.weight.data.normal_(0.0, 0.02)
                    m.bias.data.fill_(0)
    
                    # Xavier Initialization
                    # ëª¨ë“ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ xavier normalë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    init.xavier_normal(m.weight.data)
                    m.bias.data.fill_(0)
                    '''
    
                    # Kaming Initialization
                    # ëª¨ë“ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ kaming he normalë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    init.kaiming_normal_(m.weight.data)
                    m.bias.data.fill_(0)
    
                # ë§Œì•½ ê·¸ ëª¨ë“ˆì´ nn.Linearì¸ ê²½ìš°
                elif isinstance(m, nn.Linear):
                    '''
                    # ì‘ì€ ìˆ«ìë¡œ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•
                    # ê°€ì¤‘ì¹˜ë¥¼ í‰ê·  0, í¸ì°¨ 0.02ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    m.weight.data.normal_(0.0, 0.02)
                    m.bias.data.fill_(0)
    
                    # Xavier Initialization
                    # ëª¨ë“ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ xavier normalë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    init.xavier_normal(m.weight.data)
                    m.bias.data.fill_(0)
                    '''
    
                    # Kaming Initialization
                    # ëª¨ë“ˆì˜ ê°€ì¤‘ì¹˜ë¥¼ kaming he normalë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    # í¸ì°¨ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
                    init.kaiming_normal_(m.weight.data)
                    m.bias.data.fill_(0)
    
        def forward(self,x):
            out = self.layer(x)
            out = out.view(batch_size,-1)
            out = self.fc_layer(out)
            return out
    
    ```
    
- Reference
    
    [ë”¥ëŸ¬ë‹ í•™ìŠµ í–¥ìƒì„ ìœ„í•œ ê³ ë ¤ ì‚¬í•­ë“¤](http://www.gisdeveloper.co.kr/?p=8443)
    
    [Pytorch-í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤](https://wjddyd66.github.io/pytorch/Pytorch-Problem/#%EA%B0%80%EC%A4%91%EC%B9%98%EC%9D%98-%EC%B4%88%EA%B9%83%EA%B0%92)
    
    [[PyTorch] ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” í•˜ê¸° (parameter initialization)](https://jh-bk.tistory.com/10)
    
    [[ CNN ] ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Weight Initialization) - PyTorch Code](https://supermemi.tistory.com/121)
    

# 3. Layer

---

## 3.1. Batch Normalization

í™œì„±í•¨ìˆ˜ì˜ ì¶œë ¥ê°’ì„ ì •ê·œí™” í•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ë°ì´í„° ë¶„í¬ê°€ ì¹˜ìš°ì¹˜ëŠ” í˜„ìƒì„ í•´ê²°í•¨ìœ¼ë¡œì¨ ê°€ì¤‘ì¹˜ê°€ ì—‰ëš±í•œ ë°©í–¥ìœ¼ë¡œ ê°±ì‹ ë  ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°. Gradient vanishing ë¬¸ì œë¥¼ ë°©ì§€í•œë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/1.BN.png">


BN layerë¥¼ ê±°ì¹˜ë©´ ë°ì´í„°ì˜ ë¶„í¬ê°€ í‰ê·  0, ë¶„ì‚° 1ì´ ë˜ë„ë¡ ì •ê·œí™”ë¥¼ í•œë‹¤. modelì´ train modeì—ëŠ” BatchNormalizationì„ ì‹¤ì‹œí•˜ê³  evaluation modeì—ëŠ” BatchNormalizationì„ ì‹¤ì‹œí•˜ì§€ ì•ŠëŠ”ë‹¤.

- Code - Pytorch
    
    ```python
    class CNN(nn.Module):
        def __init__(self):
            super(CNN,self).__init__()
            self.layer = nn.Sequential(
                nn.Conv2d(1,16,3,padding=1),  # 28 x 28
                nn.BatchNorm2d(16),
                nn.ReLU(),
                nn.Conv2d(16,32,3,padding=1), # 28 x 28
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.MaxPool2d(2,2),            # 14 x 14
                nn.Conv2d(32,64,3,padding=1), # 14 x 14
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(2,2)             #  7 x 7
            )
            self.fc_layer = nn.Sequential(
                nn.Linear(64*7*7,100),
                nn.BatchNorm1d(100),
                nn.ReLU(),
                nn.Linear(100,10)
            )
    
        def forward(self,x):
            out = self.layer(x)
            out = out.view(batch_size,-1)
            out = self.fc_layer(out)
            return out
    ```
    

## 3.2. Drop out

**Overfitting** ì„ ë§‰ê¸°ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµì¤‘ì¼ ë•Œ, ëœë¤í•˜ê²Œ ë‰´ëŸ°ì„ êº¼ì„œ í•™ìŠµí•œë‹¤. í•™ìŠµì´ í•™ìŠµìš© ë°ì´í„°ë¡œ ì¹˜ìš°ì¹˜ëŠ” í˜„ìƒì„ ë§‰ì•„ì¤€ë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/2.DropOut.png">

pytorchì—ì„œëŠ” model.train()ìœ¼ë¡œ ëª¨ë¸ ì „ì²´ì— ìˆëŠ” Dropoutì„ ì ìš©í•œë‹¤. model.eval()ì—ì„œëŠ” Dropoutì„ ì ìš©í•˜ì§€ ì•Šê³  ëª¨ë“  ë‰´ëŸ°ì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡í•œë‹¤.

Dropoutì˜ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ê³  í•´ì„œ í•­ìƒ ê²°ê³¼ê°€ ì¢‹ì•„ì§€ì§„ ì•ŠëŠ”ë‹¤. overfitting í•˜ì§€ ì•ŠëŠ” ìƒíƒœì—ì„œ ì ìš©í•˜ë©´ ì˜¤íˆë ¤ í•™ìŠµì´ ì˜ ì•ˆë˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.

- Code
    
    ```python
    class CNN(nn.Module):
        def __init__(self):
            super(CNN,self).__init__()
            self.layer = nn.Sequential(
                nn.Conv2d(1,16,3,padding=1),  # 28
                nn.ReLU(),
                nn.Dropout2d(0.2),
                nn.Conv2d(16,32,3,padding=1), # 28
                nn.ReLU(),
                nn.Dropout2d(0.2),
                nn.MaxPool2d(2,2),            # 14
                nn.Conv2d(32,64,3,padding=1), # 14
                nn.ReLU(),
                nn.Dropout2d(0.2),
                nn.MaxPool2d(2,2)             # 7
            )
            self.fc_layer = nn.Sequential(
                nn.Linear(64*7*7,100),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(100,10)
            )       
            
        def forward(self,x):
            out = self.layer(x)
            out = out.view(batch_size,-1)
            out = self.fc_layer(out)
            return out
    ```
    

# 4. Scheduler

---

## 4.1. Idea

modelì„ í•™ìŠµí•  ë•Œ ì •í•´ì¤„ ìˆ˜ ìˆëŠ” parameter ì¤‘ í•˜ë‚˜ì¸ learning rateëŠ” í•™ìŠµì— ë§ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ì ì ˆí•œ learning rateë¥¼ ì„ íƒí•´ì•¼ ëª¨ë¸ì´ ì •ì²´ë˜ì§€ ì•Šê³  lossë¥¼ ë‚®ì¶œ ìˆ˜ ìˆë‹¤.

- ë„ˆë¬´ í¬ê²Œ ë˜ë©´ ë°œì‚°í•œë‹¤.
- ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµí•˜ëŠ”ë° ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/3.Scheduler.png">

learning rateë¥¼ í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ Learning Rateë¥¼ í¬ê²Œ ì„¤ì •í•˜ê³  ì ì°¨ ì¤„ì—¬ê°€ëŠ” ë°©ì‹ì„ ì„ íƒí•˜ê¸°ë„ í•œë‹¤.

Pytorch ì—ì„œëŠ” Learning rateë¥¼ í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ ì ì°¨ ë–¨ì–´ëœ¨ë¦¬ëŠ” ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‹¤.

`torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)`

## 4.2.Methods

### 1) Pytorch ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” scheduler

- Code
    
    ```python
    from torch.optim import lr_scheduler
    
    # 1. ë¨¼ì € model, optimizer ì„ ì–¸
    model = CNN().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    
    # 2. scheduler ì„ ì–¸
    ## step size ë‹¨ìœ„ë¡œ í•™ìŠµë¥ ì— ê°ë§ˆë¥¼ ê³±í•œë‹¤.
    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)
    
    ## ì§€ì •í•œ step ì§€ì ë§ˆë‹¤ í•™ìŠµë¥ ì— ê°ë§ˆë¥¼ ê³±í•œë‹¤.
    scheduler = lr_scheduler.MultiStepLR(optimizer,milestones=[10,30,80], gamma=0.99)
    
    ## ë§¤ epochë§ˆë‹¤ í•™ìŠµë¥ ì— ê°ë§ˆë¥¼ ê³±í•´ì¤€ë‹¤.
    scheduler = lr_scheduler.ExponentialLR(optimizer,gamma= 0.99)
    
    # ìœ„ ë°©ë²•ë“¤ì€ scheduler.step() ì´ í•„ìš”í•˜ì§€ ì•Šì•„ë³´ì„... -> ì²´í¬ í•„ìš”
    ```
    

### 2) LambdaLR

Lambda í‘œí˜„ì‹ìœ¼ë¡œ ì‘ì„±í•œ í•¨ìˆ˜ë¥¼ í†µí•´ learning rateë¥¼ ì¡°ì ˆí•œë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/4.LambdaLR.png">
<img src="/assets/images/posts_img/2022-11-12-Train-Skill/5.Lambda.png">

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
# scheduler.step() í˜¸ì¶œ ë  ë•Œë§ˆë‹¤ lambdaê°€ ì ìš©ëœë‹¤.
scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,
							 lr_lambda=lambda epoch:0.95**epoch)
```

### 3) MultiplicativeLR

Lambda í‘œí˜„ì‹ìœ¼ë¡œ ì‘ì„±í•œ í•¨ìˆ˜ë¥¼ í†µí•´ learning rateë¥¼ ì¡°ì ˆí•œë‹¤. ì´ˆê¸° learning rateì— lambdaí•¨ìˆ˜ì—ì„œ ë‚˜ì˜¨ ê°’ì„ **ëˆ„ì ê³±**í•´ì„œ learning rateë¥¼ ê³„ì‚°í•œë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/6.MultiLR.png">
<img src="/assets/images/posts_img/2022-11-12-Train-Skill/7.Multi.png">

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer=optimizer,
               lr_lambda=lambda epoch: 0.95 ** epoch)
```

### 4) CosineAnnealingLR

learning rateê°€ cos í•¨ìˆ˜ë¥¼ ë”°ë¼ì„œ eat_min ê¹Œì§€ ë–¨ì–´ì¡Œë‹¤ê°€ ë‹¤ì‹œ ì´ˆê¸° learning rateê¹Œì§€ ì˜¬ë¼ì˜¨ë‹¤.

<img src="/assets/images/posts_img/2022-11-12-Train-Skill/8.CosineLR.png">
<img src="/assets/images/posts_img/2022-11-12-Train-Skill/9.Cosine.png">

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)
```

> T_max : ìµœëŒ€ iteration íšŸìˆ˜
> 

**ì´ ì™¸ì—ë„ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆë‹¤.** 

**ì‚¬ì‹¤ ì–¸ì œ ì–´ë–¤ schedulerë¥¼ ì‚¬ìš©í•´ì•¼ ì ì ˆí•œì§€ ê°ì´ ì•ˆì¡íˆë¯€ë¡œ ì´ì •ë„ë¡œ ì •ë¦¬í•œë‹¤.**


[[PyTorch] PyTorchê°€ ì œê³µí•˜ëŠ” Learning rate scheduler ì •ë¦¬](https://sanghyu.tistory.com/113)
    

## 4.3. Print learning rate

```python
optimizer.state_dict()['param_groups'][0]['lr']
```

# 5. Multi GPU

---

í•™ìŠµì˜ ì„±ëŠ¥ ê°œì„  ë³´ë‹¤ëŠ” í•™ìŠµì˜ ì†ë„ ê°œì„ ì„ ìœ„í•œ ë°©ë²•ì´ë‹¤.


[ğŸ”¥PyTorch Multi-GPU í•™ìŠµ ì œëŒ€ë¡œ í•˜ê¸°](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)
    

# 6. Tips for stable GAN training

---

    
[https://flonelin.wordpress.com/2020/05/20/%EC%95%88%EC%A0%95%EC%A0%81%EC%9D%B8-generative-adversarial-network-%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%81%EB%93%A4/](https://flonelin.wordpress.com/2020/05/20/%EC%95%88%EC%A0%95%EC%A0%81%EC%9D%B8-generative-adversarial-network-%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%8C%81%EB%93%A4/)
    

# 7. Accumulation

---

## 7.1. Overview

Deep learning ê¸°ìˆ ì´ ë°œë‹¬í•˜ë©´ì„œ modelì˜ ìš©ëŸ‰ì´ ì»¤ì§€ê³  ìˆë‹¤. íŠ¹íˆ computer visionì˜ high-resolution image ìƒì„±ì„ ìœ„í•´ì„œëŠ” GPU memoryì— ì‹ ê²½ì„ ì“°ì§€ ì•Šì„ ìˆ˜ ì—†ë‹¤. GPU memoryìš©ëŸ‰ì— ë§ì¶° í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë˜ë©´ small batch sizeë¡œ ì„¤ì •í•  ê²ƒì´ê³  ê²°êµ­ í•™ìŠµì— ì•…ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ â€˜gradient accumulationâ€™ì„ ì†Œê°œí•œë‹¤. ê°„ë‹¨í•˜ê²Œ ì´ì•¼ê¸° í•˜ë©´, gradient accumulationì€ small batch sizeë¥¼ ì´ìš©í•˜ì§€ë§Œ gradientsë¥¼ ì €ì¥í•˜ê³  network weightë¥¼ batches ê°„ê²©ìœ¼ë¡œ í•œë²ˆì— ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•ì´ë‹¤.

## 7.2. What is gradient accumulation

### ì¼ë°˜ì ì¸ í•™ìŠµ ê³¼ì •

1. ë°ì´í„°ë¥¼ mini-batchesë¡œ ë‚˜ëˆˆë‹¤.
2. í•œ batchì”© Neural networkì— í†µê³¼ì‹œí‚¨ë‹¤.
3. NetworkëŠ” batch size ë§Œí¼ labelì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.
4. lossë¥¼ ê³„ì‚°í•œë‹¤.
5. backward passì§„í–‰
6. update model weights

### Gradient accumulation

ìœ„ ë§ˆì§€ë§‰ ê³¼ì •ì—ì„œ every batchë§ˆë‹¤ weightsë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ëŒ€ì‹ ì—, gradient valuesë¥¼ ì €ì¥í•˜ê³  ë‹¤ìŒ batchë¡œ ë„˜ì–´ê°„ë‹¤. ë‹¤ìŒ batchì—ì„œ ìƒˆë¡œìš´ gradientë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ì´ë‹¤. Weight updateëŠ” ëª‡ batch ì§„í–‰í•œ ë‹¤ìŒì— ì§„í–‰í•œë‹¤.

Gradient accumulationì€ larger batch sizeë¥¼ ì´ìš©í•œ Network í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•´ì¤€ë‹¤. ë§Œì•½ ë°°ì¹˜ë‹¹ 32ê°œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤ê³  ê°€ì •í•œë‹¤. ê·¸ëŸ¬ë‚˜ ë‚´ ì»´í“¨í„°ëŠ” ë©”ëª¨ë¦¬ ì‚¬ì •ìœ¼ë¡œ ë°°ì¹˜ë‹¹ 8ê°œ ì´ë¯¸ì§€ ë°–ì— ìˆ˜ìš©í•˜ì§€ ëª»í•œë‹¤. ì´ ê²½ìš° ë°°ì¹˜ë¥¼ 8ê°œë¡œ ì„¤ì •í•˜ê³  4ë²ˆ ë°°ì¹˜ê°€ ì§„í–‰ë˜ë©´ ê·¸ë•Œ weight updateë¥¼ ì§„í–‰ í•´ ì¤€ë‹¤. 

## 7.3. How to make it work

- `batch idx`ê°€ `accum_iter`ë¡œ ë‚˜ëˆ„ì–´ ì§ˆ ë•Œ, data loaderê°€ ë§ˆì§€ë§‰ ê¹Œì§€ loadí•˜ì˜€ì„ ë•Œ ì´ ë‘ê°€ì§€ ê²½ìš° gradient accumulationì„ ì§„í–‰í•œë‹¤.
- lossë¥¼ `accum_iter` ë¡œ ë‚˜ëˆ ì„œ normalizeë¥¼ ì§„í–‰í•œë‹¤.

```python
# batch accumulation parameter
accum_iter = 4  

# loop through enumaretad batches
for batch_idx, (inputs, labels) in enumerate(data_loader):

    # extract inputs and labels
    inputs = inputs.to(device)
    labels = labels.to(device)

    # passes and weights update
    with torch.set_grad_enabled(True):
        
        # forward pass 
        preds = model(inputs)
        loss  = criterion(preds, labels)

        # normalize loss to account for batch accumulation
        loss = loss / accum_iter 

        # backward pass
        loss.backward()

        # weights update
        if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(data_loader)):
            optimizer.step()
            optimizer.zero_grad()
```

## 7.4. Closing words

- always recommend using gradient accumulation when working with large architectures that consume a lof of GPU memory.