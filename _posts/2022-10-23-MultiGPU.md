---
title: "Multi GPU"
excerpt: "DataParallel & DistributedDataParallel"

categories:
  - DL
tags:
  - [GPU]
  - [DL] 
  - [Multi GPU]
  - [train]

permalink: /DL/Multi_GPU/

toc: true
toc_sticky: true

date: 2022-10-23
last_modified_at: 2022-10-23
---

# 1. DataParallelì„ ì´ìš©í•œ Multi GPU

## 1.1. How to use it?

```python
device = torch.device("cuda:0")
model.to(device)

mytensor = my_tensor.to(device)

model = nn.DataParallel(model)
```

## 1.2. Step

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/1.step.png">


1. 1 GPUì— ëª¨ë¸ì„ í•˜ë‚˜ ë§Œë“ ë‹¤.(default GPU model)
2. ê° GPUì— ëª¨ë¸ë“¤ì„ ë§Œë“ ë‹¤.
    - 1 GPUì—ëŠ” ëª¨ë¸ì´ 2ê°œ ìƒì„±ëœë‹¤.
3. ë°ì´í„°ë¥¼ batch sizeë§Œí¼ loadí•˜ê³  GPU ê°œìˆ˜ë§Œí¼ ë‚˜ëˆˆë‹¤.(mini batch ìƒì„±)
4. mini batchë¥¼ ê° GPUì—ê²Œ ì „ë‹¬í•œë‹¤.
5. ê° GPUì— ìˆëŠ” modelì—ì„œ Feed Forward, backpropagationë¥¼ ì§„í–‰í•˜ì—¬ gradientê¹Œì§€ ê³„ì‚°í•œë‹¤.
6. default GPUì— gradientë¥¼ í•©í•˜ê³  ì´ê±¸ë¡œ default GPUì˜ modelì„ updateí•œë‹¤.
7. ë‹¤ìŒ í•™ìŠµ ìŠ¤í…ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´ì„œ default modelì˜ parameterë¥¼ ê° GPUì— ì „ë‹¬í•œë‹¤.
8. ë°˜ë³µí•˜ì—¬ í•™ìŠµí•œë‹¤.

## 1.3. Results

[ 1 GPU ]

- input : 30ê°œ
- each GPUâ€™s output : 30ê°œ

[ 2 GPU ]

- input : 30ê°œ
- each GPUâ€™s output : 15ê°œ

[ 3 GPU ]

- input : 30ê°œ
- each GPUâ€™s output : 10ê°œ

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/2.nvidia-smi.png">


ì •ë¦¬í•˜ìë©´ 1 GPUì—ëŠ” ë™ì¼í•œ ëª¨ë¸ì´ 2ê°œê°€ ì„ ì–¸ë˜ì–´ ìˆê³ , dataë¥¼ batch size ë§Œí¼ loadí•  ë•Œë„ 1 GPUì—ì„œ ì§„í–‰ëœë‹¤. ê·¸ë˜ì„œ ë°‘ì˜ ì‚¬ì§„ì²˜ëŸ¼ 1ë²ˆ GPUì— ë§ì€ GPU memoryê°€ í• ë‹¹ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê²Œë‹¤ê°€ ê° GPU ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.

Reference
    
[How PyTorch implements DataParallel?](https://erickguan.me/2019/pytorch-parallel-model)

[ì„ íƒ ì‚¬í•­: ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ (Data Parallelism)](https://tutorials.pytorch.kr/beginner/blitz/data_parallel_tutorial.html)

[ë©€í‹°-GPU ì˜ˆì œ](https://tutorials.pytorch.kr/beginner/former_torchies/parallelism_tutorial.html)

[ğŸ”¥PyTorch Multi-GPU í•™ìŠµ ì œëŒ€ë¡œ í•˜ê¸°](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)
    

# 2. DistributedDataParallelì„ ì´ìš©í•œ multi GPU í•™ìŠµ

## 2.1. Overview DDP

### Terms

- master node : synchronizationí•  main GPUë¥¼ ì˜ë¯¸í•œë‹¤. *making copies, loading models, writing logs*
- process group : Kê°œ GPUë¥¼ train/test í•˜ê¸° ì›í•œë‹¤ë©´ 1ê°œ groupì— Kê°œì˜ process ê°€ í•„ìš”í•˜ë‹¤. backendì—ì„œ ê´€ë¦¬ë˜ë©° pytorchì—ì„œëŠ” **nccl** ì„ ì‚¬ìš©í•œë‹¤.
    - groupì€ í•œ train/test ì‘ì—…ì„ ì´ì•¼ê¸° í•˜ëŠ” ê²ƒ ê°™ë‹¤.
- rank : í•œ process groupì— ìˆëŠ” process ë§ˆë‹¤ identity rankë¥¼ ë§¤ê¸´ë‹¤. 0 ~ k - 1 ê¹Œì§€ ì¡´ì¬í•œë‹¤.
- world wise : groupì•ˆì— ì¡´ì¬í•˜ëŠ” processes ìˆ«ìë¥¼ ì˜ë¯¸í•œë‹¤.(i.e. gpu number == K)

## 2.2. DataParallel vs DistributedDataParallel

Multi GPUë¥¼ ì‚¬ìš©í•˜ëŠ” í•™ìŠµ ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤. DataParallel(DP) ì™€ DistributedDataParallel(DDP)ê°€ ê·¸ê²ƒë“¤ì´ë‹¤. ê³µì‹ì ìœ¼ë¡œ ì¶”ì²œí•˜ëŠ” ë°©ë²•ì€ í›„ìì´ë‹¤. 

DDP ê°€ ë” ë¹ ë¥´ê³  ìœ ì—°í•˜ë‹¤. DDPëŠ” multiple gpusì— modelì„ ë³µì‚¬í•œë‹¤.

- graidents ê°’ì„ modelë¡œ ë¶€í„° ì–»ê³  í‰ê·  graidentë¥¼ modelì— ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì´ë‹¤.
- ê·¸ë¦¬ê³  Kê°œ processesì˜ modelì„ synchronizeí•œë‹¤.

## 2.3. Overall process

### 1. Prerequisite

ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°±ì—”ë“œë¥¼ í˜¸ì¶œí•˜ëŠ” ê³¼ì •

ì¼ë°˜ì ìœ¼ë¡œ ncclì´ ì‚¬ìš©ëœë‹¤.(ì„±ëŠ¥ ë° ì†ë„ê°€ ë” ì¢‹ë‹¤.)

ì´ ê³¼ì •ì„ í†µí•´ ê° gpuë³„ë¡œ spawned processê°€ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì— ë“±ë¡ë˜ê³  ì´ë¥¼ í†µí•´ broadcast/all-reduce ë“±ì˜ collective communicationì´ ê°€ëŠ¥í•´ ì§„ë‹¤.

- ê° gpuë³„ë¡œ rankë¥¼ ì„¤ì •í•œë‹¤
- world sizeë¥¼ í†µí•´ì„œ ì „ì²´ gpuê°œìˆ˜ë¥¼ ì•Œë ¤ì¤€ë‹¤.

```python
import torch.distributed as dist

dist.init_process_group("nccl", rank=rank, world_size=world_size)
```

### 2. Construction

`torch.nn.parallel.DistributedDataParallel` í•¨ìˆ˜ë¥¼ í†µí•´ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ìƒì„±ëœ ëª¨ë¸ì„ DDP ëª¨ë¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê³¼ì •

```python
from torch.nn.parallel import DistributedDataParallel as DDP

model = DDP(model, device_id=[rank])
```

DDP êµ¬ë¬¸ì´ ìœ„ ì—­í• ì„ ë‹´ë‹¹í•œë‹¤.

master rankì¸ rank 0ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ë¥¸ rankë¡œ broadcast í•¨ìœ¼ë¡œì¨ ê° í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë¸ ë³µì‚¬ë³¸ì´ ê°™ì€ ìƒíƒœì—ì„œ ì‹œì‘í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

ì´í›„ ê° rankì—ì„œ ìƒì„±ëœ DDPëª¨ë¸ì€ backward passì—ì„œ ê³„ì‚°ëœ gradientì˜ ë™ê¸°í™”ë¥¼ ìœ„í•œ **Reducer** ê°ì²´ë¥¼ ìƒì„±í•œë‹¤. ReducerëŠ” íš¨ìœ¨ì ì¸ collective communicationì„ ìœ„í•´ parameter gradientsì˜ ë¬¶ìŒì¸ **bucket**ì„ ìƒì„±í•œë‹¤.

ë‹¤ì‹œ ë§í•˜ìë©´, ê° gpuì—ì„œ back propagationìœ¼ë¡œ ê³„ì‚°ëœ gradientê°€ ìˆê³ , ì´ gradientë“¤ì€ bucket ë‹¨ìœ„ë¡œ all-reduce ê³¼ì •ì´ ìˆ˜í–‰ëœë‹¤. 

(all-reduceëŠ” ê° Processì˜ gradientì˜ í‰ê· ì„ êµ¬í•´ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.)

(ê° í”„ë¡œì„¸ìŠ¤ë³„ë¡œ bucketì´ ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ê³ , all-reduceëŠ” í”„ë¡œì„¸ìŠ¤ ë³„ ì¡´ì¬í•˜ëŠ” ê³µí†µ bucketë¼ë¦¬ ìˆ˜í–‰ëœë‹¤.)

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/3.bucket.png">


### 3. Forward pass

ì¼ë°˜ì ì¸ neural networksì˜ forward pass

### 4. Backward pass

ê° í”„ë¡œì„¸ìŠ¤ ë³„ë¡œ lossê°€ ê³„ì‚°ì´ ë˜ë©´ 2ë²ˆ construction ë‹¨ê³„ì—ì„œ ë“±ë¡í•œ gradient hookì„ ì´ìš©í•˜ì—¬ ê° í”„ë¡œì„¸ìŠ¤ì˜ í•´ë‹¹ë˜ëŠ” íŒŒë¼ë¯¸í„° ë³„ gradientë¥¼ all-reduce ê³¼ì •ì„ í†µí•´ í•©í•˜ê³  ë™ê¸°í™”í•œë‹¤.

1. ê° íŒŒë¼ë¯¸í„°ì˜ gradientê°€ ê³„ì‚°ì´ ë˜ë©´ hookê°€ ë°œë™ëœë‹¤.
2. í•œ bucketì— ì†í•œ gradientê°€ ëª¨ë‘ ê³„ì‚°ì´ ë˜ë©´ ì™„ë£Œëœ bucket ë³„ë¡œ all-reduce ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.
3. ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê³„ì‚°ëœ í•´ë‹¹ bucketì— ì†í•œ íŒŒë¼ë¯¸í„°ì˜ gradientë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í•©ì‚°í•œë‹¤.
4. ëª¨ë“  bucketì˜ gradient ê³„ì‚°ì´ ì™„ë£Œë˜ë©´ ë™ê¸°í™”ë¥¼ ìœ„í•´ ëª¨ë“  bucketì˜ all-reduce ê³¼ì •ì´ ì™„ë£Œë˜ê¸°ê¹Œì§€ block
5. ì´ ê³¼ì •ì´ ëë‚˜ë©´ ê° íŒŒë¼ë¯¸í„°ì˜ í‰ê·  gradients(all-reduceë¥¼ í†µí•´ ê³„ì‚°í•œ gradients)ê°€ param.gradì— ë“±ë¡ëœë‹¤.

(ê°„ë‹¨í•˜ê²Œ ë§í•´ bucketë³„ë¡œ all-reduceì„ êµ¬í•˜ê³ , bucket gradient í‰ê· ì„ ê³µìœ í•œë‹¤.)

### 5. Optimizing step

## 2.4. Operation

### 1. Setup

ì‹¤ë¬´ìê°€ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ì™€ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ê¸°ì—ì„œ ê³„ì‚°ì„ ì‰½ê²Œ ë³‘ë ¬í™” í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ì´ë¥¼ ìœ„í•´, ê° í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì™€ ë°ì´í„°ë¥¼ êµí™˜í•  ìˆ˜ ìˆë„ë¡ ë©”ì‹œì§€ êµí™˜ ê·œì•½(messaging passing semantics)ì„ í™œìš©í•œë‹¤. 

ë©€í‹°í”„ë¡œë ˆì‹±(`torch.multiprocessing`) íŒ¨í‚¤ì§€ì™€ ë‹¬ë¦¬, í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ë¥¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë°±ì—”ë“œ(backend)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.(â€™ncclâ€™, â€™glooâ€™ â€¦)

ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ

- Code
    
    ```python
    """run.py:"""
    #!/usr/bin/env python
    import os
    import torch
    import torch.distributed as dist
    import torch.multiprocessing as mp
    
    def run(rank, size):
        """ Distributed function to be implemented later. """
        pass
    
    def init_process(rank, size, fn, backend='gloo'):
        """ Initialize the distributed environment. """
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
        dist.init_process_group(backend, rank=rank, world_size=size)
        fn(rank, size)
    
    if __name__ == "__main__":
        size = 2
        processes = []
        for rank in range(size):
            p = mp.Process(target=init_process, args=(rank, size, run))
            p.start()
            processes.append(p)
    		# ë‹¤ë¥¸ processê°€ ë§ˆë¬´ë¦¬ ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤.
        for p in processes:
            p.join()
    ```
    
    2ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒì„±(spawn)í•˜ì—¬ ê°ì ë‹¤ë¥¸ ë¶„ì‚° í™˜ê²½ì„ ì„¤ì •í•˜ê³ , í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹(`dist.init_process_group`)ì„ ì´ˆê¸°í™”í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ run í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•œë‹¤.
    
    [ init process ]
    
    ë™ì¼í•œ IPì£¼ì†Œì™€ í¬íŠ¸ë¥¼ í†µí•´ ë§ˆìŠ¤í„°ë¥¼ ì§€ì •í•˜ê³ , ë§ˆìŠ¤í„°ë¥¼ í†µí•´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¡°ì •(coordinate)ë  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
    

### 2. Point-to-Point í†µì‹ 

í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” ê²ƒ. ì§€ì ê°„ í†µì‹ ì„ ìœ„í•´ì„œ `send`, `recv` í•¨ìˆ˜ ë˜ëŠ” ì¦‰ì‹œ ì‘ë‹µí•˜ëŠ” `isend` ì™€ `irecv` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/4.p2p.png">


[ send ]

```python
torch.distributed.send(tensor, dist, group=None, tag=0)
```

- tensor : ë³´ë‚¼ tensor
- dist : destination rank

[ recv ]

```python
torch.distributed.recv(tensor, src, group, tag)
```

- tensor : ë°›ì€ ë°ì´í„°ë¥¼ ì €ì¥í•  ë³€ìˆ˜
- src : source rank. ì§€ì •í•˜ì§€ ì•Šê³  ì–´ë””ë“ ì§€ ë°›ì•„ë„ ê´œì°®ë‹¤ë©´ ê³µë€
- Code
    
    ```python
    """ë¸”ë¡œí‚¹(blocking) ì -ëŒ€-ì  ê°„ í†µì‹ """
    
    def run(rank, size):
        tensor = torch.zeros(1)
        tensor2 = torch.zeros(1)
        if rank == 0:
            tensor += 2
            # Send the tensor to process 1
            dist.send(tensor=tensor, dst=1)
        else:
            # Receive tensor from process 0
            dist.recv(tensor=tensor2, src=0)
        print('Rank ', rank, ' has data ', tensor[0], tensor2[0])
        pass
    ```
    
    ë‘ í”„ë¡œì„¸ìŠ¤ëŠ” ê°’ì´ 0ì¸ tensorë¡œ ì‹œì‘í•œ í›„, 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ tensorì˜ ê°’ì„ 1 ì¦ê°€ì‹œí‚¨ í›„ 1ë²ˆ í”„ë¡œì„¸ìŠ¤ë¡œ ê°’ì„ ì „ì†¡í•˜ì—¬ ë‘ í”„ë¡œì„¸ìŠ¤ ë‹¤ 1ë¡œ tensor ê°’ì´ ì €ì¥ëœë‹¤.
    
    ì´ ë•Œ, í”„ë¡œì„¸ìŠ¤ 1ì€ ìˆ˜ì‹ í•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹í•´ë‘ì–´ì•¼ í•œë‹¤.
    

[ isend ]

```python
torch.distributed.isend(tensor, dst, group, tag)
```

[ irecv ]

```python
torch.distributed.irecv(tensor, src, group, tag)
```

- Code
    
    ```python
    """ë…¼-ë¸”ë¡œí‚¹(non-blocking) ì -ëŒ€-ì  ê°„ í†µì‹ """
    
    def run(rank, size):
        tensor = torch.zeros(1)
        req = None
        if rank == 0:
            tensor += 1
            # Send the tensor to process 1
            req = dist.isend(tensor=tensor, dst=1)
            print('Rank 0 started sending')
        else:
            # Receive tensor from process 0
            req = dist.irecv(tensor=tensor, src=0)
            print('Rank 1 started receiving')
        req.wait()
        print('Rank ', rank, ' has data ', tensor[0])
    ```
    
    ì¦‰ì‹œ ì‘ë‹µí•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•  ë•ŒëŠ” tensorë¥¼ ì–´ë–»ê²Œ ì£¼ê³  ë°›ì„ì§€ë¥¼ ì£¼ì˜í•´ì•¼ í•œë‹¤. ë°ì´í„°ê°€ ì–¸ì œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì†¡ìˆ˜ì‹ ë˜ëŠ”ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— `req.wait()`ê°€ ì™„ë£Œë˜ê¸° ì „ê¹Œì§€ëŠ” ì „ì†¡ëœ tensorë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ìˆ˜ì‹ ëœ tensorì— ì ‘ê·¼í•´ì„œëŠ” ì•ˆëœë‹¤.
    

### 3. ì§‘í•© í†µì‹ (Collective Communication)

ì§‘í•© í†µì‹ ì€ **ê·¸ë£¹**ì˜ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ê±¸ì¹œ í†µì‹  íŒ¨í„´ì„ í—ˆìš©í•œë‹¤.

- ê·¸ë£¹ ìƒì„± : `dist.new_group(group)` ì— ìˆœì„œ(rank) ëª©ë¡ì„ ì „ë‹¬í•œë‹¤.
- ì›”ë“œ(world) : ì§‘í•© í†µì‹ ì´ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜
    - ì˜ˆì‹œ : ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  tensorì˜ í•©ì„ ì–»ê¸°
        
        `dist.all_reduce(tensor, op, group)`
        

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/5.Communication.png">


[ all reduce ]

```python
torch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group, async_op)
```

- groupì— ìˆëŠ” rankë“¤ì„ ëª¨ë‘ ë”í•˜ë©´ì„œ rankì— ê°’ì„ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ
- ìë™ìœ¼ë¡œ groupì— ìˆëŠ” â€˜tensorâ€™ ë³€ìˆ˜ì— ì €ì¥ëœ ê°’ì„ ë”í•œ ë’¤ì— ëŒ€ì¹˜ì‹œí‚¨ë‹¤.

[ reduce ]

```python
torch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group, async_op)
```

- reduceëŠ” ì €ì¥ë˜ëŠ” rankê°€ ì •í•´ì ¸ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— dst parameterê°€ ì¡´ì¬í•œë‹¤.
- dst rankì˜ â€˜tensorâ€™ ë³€ìˆ˜ì— op ê³„ì‚° ê°’ì„ ì €ì¥í•´ ì¤€ë‹¤.
- Code
    
    ```python
    """ All-Reduce ì˜ˆì œ """
    def run(rank, size):
        """ ê°„ë‹¨í•œ ì§‘í•© í†µì‹  """
        group = dist.new_group([0, 1])
        tensor = torch.ones(1)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)
        print('Rank ', rank, ' has data ', tensor[0])
    ```
    
    - ê·¸ë£¹ ë‚´ì˜ ëª¨ë“  tensorë“¤ì˜ í•©ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— `dist.ReduceOp.SUM` ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
    - `dist.ReduceOp.SUM` / `dist.ReduceOp.PRODUCT` / `dist.ReduceOp.MAX` / `dist.ReduceOp.Min`

## 2.5. ë¶„ì‚° í•™ìŠµ(Distributed Training)

- SGDì˜ ë¶„ì‚° ë²„ì „ êµ¬í˜„

### 1) Dataset ì„ ì–¸

- ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ê°ìì˜ ë°ì´í„° ë°°ì¹˜(batch)ì—ì„œ ê°ìì˜ ëª¨ë¸ì˜ ë³€í™”ë„(gradient)ë¥¼ ê³„ì‚°í•œ í›„ í‰ê· ì„ ê³„ì‚°í•œë‹¤. í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜ë¥¼ ë³€ê²½í•´ë„ ìœ ì‚¬í•œ ìˆ˜ë ´ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´ì„œ, ë°ì´í„°ì…‹ì„ ë¶„í• í•´ì•¼ í•œë‹¤.
- Code
    
    ```python
    """ ë°ì´í„°ì…‹ ë¶„í•  í—¬í¼(helper) """
    class Partition(object):
    
        def __init__(self, data, index):
            self.data = data
            self.index = index
    
        def __len__(self):
            return len(self.index)
    
        def __getitem__(self, index):
            data_idx = self.index[index]
            return self.data[data_idx]
    
    class DataPartitioner(object):
    
        def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):
            self.data = data
            self.partitions = []
            rng = Random()
            rng.seed(seed)
            data_len = len(data)
            indexes = [x for x in range(0, data_len)]
            rng.shuffle(indexes)
    
            for frac in sizes:
                part_len = int(frac * data_len)
                self.partitions.append(indexes[0:part_len])
                indexes = indexes[part_len:]
    
        def use(self, partition):
            return Partition(self.data, self.partitions[partition])
    ```
    
    - `DataPartitioner.__init__`
        - í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜(rank ê°œìˆ˜)ë§Œí¼ datasetì„ partitions listì— ë‚˜ëˆ ì¤€ë‹¤.
        - partitions listì—ëŠ” partitionìœ¼ë¡œ ë‚˜ëˆ ì§„ index ê°’ì´ ì¡´ì¬í•œë‹¤.
    - `DataPartitioner.use`
        - rank ê°œìˆ˜ë¡œ ë‚˜ëˆ ì§„ partitionsì„ ì‚¬ìš©í•œë‹¤.
        - ì‹¤í–‰ë˜ëŠ” rank indexë¥¼ í†µí•´ dataset ì¼ë¶€ë¶„ì„ ì‚¬ìš©í•œë‹¤.

### 2) DataLoader

- Code
    
    ```python
    """ MNIST ë°ì´í„°ì…‹ ë¶„í•  """
    def partition_dataset():
        dataset = datasets.MNIST('./data', train=True, download=True,
                                 transform=transforms.Compose([
                                     transforms.ToTensor(),
                                     transforms.Normalize((0.1307,), (0.3081,))
                                 ]))
        size = dist.get_world_size()
        bsz = 128 / float(size)
        partition_sizes = [1.0 / size for _ in range(size)]
        partition = DataPartitioner(dataset, partition_sizes)
        partition = partition.use(dist.get_rank())
        train_set = torch.utils.data.DataLoader(partition,
                                             batch_size=bsz,
                                             shuffle=True)
        return train_set, bsz
    ```
    
    - datasetì— image paths ë¥¼ ì „ë‹¬í•˜ë©´ ë ë“¯

### 3) Run

- ì¼ë°˜ì ì¸ í•™ìŠµ ê³¼ì •ê³¼ ë¹„ìŠ·í•˜ë‹¤

```python
""" ë¶„ì‚° ë™ê¸°(synchronous) SGD ì˜ˆì œ """
def run(rank, size):
    torch.manual_seed(1234)
    train_set, bsz = partition_dataset()
    model = Net()
    optimizer = optim.SGD(model.parameters(),
                          lr=0.01, momentum=0.5)

    num_batches = ceil(len(train_set.dataset) / float(bsz))
    for epoch in range(10):
        epoch_loss = 0.0
        for data, target in train_set:
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            epoch_loss += loss.item()
            loss.backward()
            average_gradients(model)
            optimizer.step()
        print('Rank ', dist.get_rank(), ', epoch ',
              epoch, ': ', epoch_loss / num_batches)
```

[ average_gradients ]

- ì™œ ë³€í™”ë„ í‰ê· ì„ ê³„ì‚°í•˜ëŠ”ì§€ ì˜ ëª¨ë¥´ê² ë‹¤.

```python
""" ë³€í™”ë„ í‰ê·  ê³„ì‚°í•˜ê¸° """
def average_gradients(model):
    size = float(dist.get_world_size())
    for param in model.parameters():
        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
        param.grad.data /= size
```

Reference

[Distributed communication package - torch.distributed - PyTorch 1.12 documentation](https://pytorch.org/docs/stable/distributed.html)

[PyTorchë¡œ ë¶„ì‚° ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œí•˜ê¸°](https://tutorials.pytorch.kr/intermediate/dist_tuto.html)

[A Comprehensive Tutorial to Pytorch DistributedDataParallel](https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)

[Pytorch - DistributedDataParallel (1) - ê°œìš”](https://hongl.tistory.com/292?category=927704)

[Pytorch - DistributedDataParallel (2) - ë™ì‘ ì›ë¦¬](https://hongl.tistory.com/293)

[Distributed data parallel training in Pytorch](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)