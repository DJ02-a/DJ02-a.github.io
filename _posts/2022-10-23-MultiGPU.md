---
title: "Multi GPU"
excerpt: "DataParallel & DistributedDataParallel"

categories:
  - DL
tags:
  - [GPU]
  - [DL] 
  - [Multi GPU]
  - [train]

permalink: /DL/Multi_GPU/

toc: true
toc_sticky: true

date: 2022-10-23
last_modified_at: 2022-10-23
---

# 1. DataParallel을 이용한 Multi GPU

## 1.1. How to use it?

```python
device = torch.device("cuda:0")
model.to(device)

mytensor = my_tensor.to(device)

model = nn.DataParallel(model)
```

## 1.2. Step

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/1.step.png">


1. 1 GPU에 모델을 하나 만든다.(default GPU model)
2. 각 GPU에 모델들을 만든다.
    - 1 GPU에는 모델이 2개 생성된다.
3. 데이터를 batch size만큼 load하고 GPU 개수만큼 나눈다.(mini batch 생성)
4. mini batch를 각 GPU에게 전달한다.
5. 각 GPU에 있는 model에서 Feed Forward, backpropagation를 진행하여 gradient까지 계산한다.
6. default GPU에 gradient를 합하고 이걸로 default GPU의 model을 update한다.
7. 다음 학습 스텝을 준비하기 위해서 default model의 parameter를 각 GPU에 전달한다.
8. 반복하여 학습한다.

## 1.3. Results

[ 1 GPU ]

- input : 30개
- each GPU’s output : 30개

[ 2 GPU ]

- input : 30개
- each GPU’s output : 15개

[ 3 GPU ]

- input : 30개
- each GPU’s output : 10개

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/2.nvidia-smi.png">


정리하자면 1 GPU에는 동일한 모델이 2개가 선언되어 있고, data를 batch size 만큼 load할 때도 1 GPU에서 진행된다. 그래서 밑의 사진처럼 1번 GPU에 많은 GPU memory가 할당된 것을 볼 수 있다. 게다가 각 GPU 마다 서로 다른 모델이 학습하는 것이기 때문에 학습이 잘 되지 않을 것이다.

Reference
    
[How PyTorch implements DataParallel?](https://erickguan.me/2019/pytorch-parallel-model)

[선택 사항: 데이터 병렬 처리 (Data Parallelism)](https://tutorials.pytorch.kr/beginner/blitz/data_parallel_tutorial.html)

[멀티-GPU 예제](https://tutorials.pytorch.kr/beginner/former_torchies/parallelism_tutorial.html)

[🔥PyTorch Multi-GPU 학습 제대로 하기](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)
    

# 2. DistributedDataParallel을 이용한 multi GPU 학습

## 2.1. Overview DDP

### Terms

- master node : synchronization할 main GPU를 의미한다. *making copies, loading models, writing logs*
- process group : K개 GPU를 train/test 하기 원한다면 1개 group에 K개의 process 가 필요하다. backend에서 관리되며 pytorch에서는 **nccl** 을 사용한다.
    - group은 한 train/test 작업을 이야기 하는 것 같다.
- rank : 한 process group에 있는 process 마다 identity rank를 매긴다. 0 ~ k - 1 까지 존재한다.
- world wise : group안에 존재하는 processes 숫자를 의미한다.(i.e. gpu number == K)

## 2.2. DataParallel vs DistributedDataParallel

Multi GPU를 사용하는 학습 방법은 크게 두가지가 존재한다. DataParallel(DP) 와 DistributedDataParallel(DDP)가 그것들이다. 공식적으로 추천하는 방법은 후자이다. 

DDP 가 더 빠르고 유연하다. DDP는 multiple gpus에 model을 복사한다.

- graidents 값을 model로 부터 얻고 평균 graident를 model에 업데이트하는 방식이다.
- 그리고 K개 processes의 model을 synchronize한다.

## 2.3. Overall process

### 1. Prerequisite

라이브러리 백엔드를 호출하는 과정

일반적으로 nccl이 사용된다.(성능 및 속도가 더 좋다.)

이 과정을 통해 각 gpu별로 spawned process가 프로세스 그룹에 등록되고 이를 통해 broadcast/all-reduce 등의 collective communication이 가능해 진다.

- 각 gpu별로 rank를 설정한다
- world size를 통해서 전체 gpu개수를 알려준다.

```python
import torch.distributed as dist

dist.init_process_group("nccl", rank=rank, world_size=world_size)
```

### 2. Construction

`torch.nn.parallel.DistributedDataParallel` 함수를 통해 각 프로세스에서 생성된 모델을 DDP 모델로 사용할 수 있게 하는 과정

```python
from torch.nn.parallel import DistributedDataParallel as DDP

model = DDP(model, device_id=[rank])
```

DDP 구문이 위 역할을 담당한다.

master rank인 rank 0의 모델 파라미터를 다른 rank로 broadcast 함으로써 각 프로세스의 모델 복사본이 같은 상태에서 시작할 수 있게 한다.

이후 각 rank에서 생성된 DDP모델은 backward pass에서 계산된 gradient의 동기화를 위한 **Reducer** 객체를 생성한다. Reducer는 효율적인 collective communication을 위해 parameter gradients의 묶음인 **bucket**을 생성한다.

다시 말하자면, 각 gpu에서 back propagation으로 계산된 gradient가 있고, 이 gradient들은 bucket 단위로 all-reduce 과정이 수행된다. 

(all-reduce는 각 Process의 gradient의 평균을 구해주는 알고리즘이다.)

(각 프로세스별로 bucket이 여러 개 존재하고, all-reduce는 프로세스 별 존재하는 공통 bucket끼리 수행된다.)

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/3.bucket.png">


### 3. Forward pass

일반적인 neural networks의 forward pass

### 4. Backward pass

각 프로세스 별로 loss가 계산이 되면 2번 construction 단계에서 등록한 gradient hook을 이용하여 각 프로세스의 해당되는 파라미터 별 gradient를 all-reduce 과정을 통해 합하고 동기화한다.

1. 각 파라미터의 gradient가 계산이 되면 hook가 발동된다.
2. 한 bucket에 속한 gradient가 모두 계산이 되면 완료된 bucket 별로 all-reduce 과정을 수행한다.
3. 각 프로세스에서 계산된 해당 bucket에 속한 파라미터의 gradient를 비동기적으로 합산한다.
4. 모든 bucket의 gradient 계산이 완료되면 동기화를 위해 모든 bucket의 all-reduce 과정이 완료되기까지 block
5. 이 과정이 끝나면 각 파라미터의 평균 gradients(all-reduce를 통해 계산한 gradients)가 param.grad에 등록된다.

(간단하게 말해 bucket별로 all-reduce을 구하고, bucket gradient 평균을 공유한다.)

### 5. Optimizing step

## 2.4. Operation

### 1. Setup

실무자가 여러 프로세스와 클러스터의 기기에서 계산을 쉽게 병렬화 할 수 있게 한다. 이를 위해, 각 프로세스가 다른 프로세스와 데이터를 교환할 수 있도록 메시지 교환 규약(messaging passing semantics)을 활용한다. 

멀티프로레싱(`torch.multiprocessing`) 패키지와 달리, 프로세스는 다른 커뮤니케이션 백엔드(backend)를 사용할 수 있다.(’nccl’, ’gloo’ …)

여러 프로세스를 생성하는 코드

- Code
    
    ```python
    """run.py:"""
    #!/usr/bin/env python
    import os
    import torch
    import torch.distributed as dist
    import torch.multiprocessing as mp
    
    def run(rank, size):
        """ Distributed function to be implemented later. """
        pass
    
    def init_process(rank, size, fn, backend='gloo'):
        """ Initialize the distributed environment. """
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
        dist.init_process_group(backend, rank=rank, world_size=size)
        fn(rank, size)
    
    if __name__ == "__main__":
        size = 2
        processes = []
        for rank in range(size):
            p = mp.Process(target=init_process, args=(rank, size, run))
            p.start()
            processes.append(p)
    		# 다른 process가 마무리 될 때까지 기다린다.
        for p in processes:
            p.join()
    ```
    
    2개의 프로세스를 생성(spawn)하여 각자 다른 분산 환경을 설정하고, 프로세스 그룹(`dist.init_process_group`)을 초기화하고 최종적으로 run 함수를 실행한다.
    
    [ init process ]
    
    동일한 IP주소와 포트를 통해 마스터를 지정하고, 마스터를 통해 프로세스를 조정(coordinate)될 수 있도록 한다. 
    

### 2. Point-to-Point 통신

하나의 프로세스에서 다른 프로세스로 데이터를 전송하는 것. 지점간 통신을 위해서 `send`, `recv` 함수 또는 즉시 응답하는 `isend` 와 `irecv` 함수를 사용한다.

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/4.p2p.png">


[ send ]

```python
torch.distributed.send(tensor, dist, group=None, tag=0)
```

- tensor : 보낼 tensor
- dist : destination rank

[ recv ]

```python
torch.distributed.recv(tensor, src, group, tag)
```

- tensor : 받은 데이터를 저장할 변수
- src : source rank. 지정하지 않고 어디든지 받아도 괜찮다면 공란
- Code
    
    ```python
    """블로킹(blocking) 점-대-점 간 통신"""
    
    def run(rank, size):
        tensor = torch.zeros(1)
        tensor2 = torch.zeros(1)
        if rank == 0:
            tensor += 2
            # Send the tensor to process 1
            dist.send(tensor=tensor, dst=1)
        else:
            # Receive tensor from process 0
            dist.recv(tensor=tensor2, src=0)
        print('Rank ', rank, ' has data ', tensor[0], tensor2[0])
        pass
    ```
    
    두 프로세스는 값이 0인 tensor로 시작한 후, 0번 프로세스가 tensor의 값을 1 증가시킨 후 1번 프로세스로 값을 전송하여 두 프로세스 다 1로 tensor 값이 저장된다.
    
    이 때, 프로세스 1은 수신한 데이터를 저장할 메모리를 할당해두어야 한다.
    

[ isend ]

```python
torch.distributed.isend(tensor, dst, group, tag)
```

[ irecv ]

```python
torch.distributed.irecv(tensor, src, group, tag)
```

- Code
    
    ```python
    """논-블로킹(non-blocking) 점-대-점 간 통신"""
    
    def run(rank, size):
        tensor = torch.zeros(1)
        req = None
        if rank == 0:
            tensor += 1
            # Send the tensor to process 1
            req = dist.isend(tensor=tensor, dst=1)
            print('Rank 0 started sending')
        else:
            # Receive tensor from process 0
            req = dist.irecv(tensor=tensor, src=0)
            print('Rank 1 started receiving')
        req.wait()
        print('Rank ', rank, ' has data ', tensor[0])
    ```
    
    즉시 응답하는 함수들을 사용할 때는 tensor를 어떻게 주고 받을지를 주의해야 한다. 데이터가 언제 다른 프로세스로 송수신되는지 모르기 때문에 `req.wait()`가 완료되기 전까지는 전송된 tensor를 수정하거나 수신된 tensor에 접근해서는 안된다.
    

### 3. 집합 통신(Collective Communication)

집합 통신은 **그룹**의 모든 프로세스에 걸친 통신 패턴을 허용한다.

- 그룹 생성 : `dist.new_group(group)` 에 순서(rank) 목록을 전달한다.
- 월드(world) : 집합 통신이 실행되는 위치
    - 예시 : 모든 프로세스에 존재하는 모든 tensor의 합을 얻기
        
        `dist.all_reduce(tensor, op, group)`
        

<img src="/assets/images/posts_img/2022-10-23-MultiGPU/5.Communication.png">


[ all reduce ]

```python
torch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group, async_op)
```

- group에 있는 rank들을 모두 더하면서 rank에 값을 나눠주는 것
- 자동으로 group에 있는 ‘tensor’ 변수에 저장된 값을 더한 뒤에 대치시킨다.

[ reduce ]

```python
torch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group, async_op)
```

- reduce는 저장되는 rank가 정해져 있어야 하기 때문에 dst parameter가 존재한다.
- dst rank의 ‘tensor’ 변수에 op 계산 값을 저장해 준다.
- Code
    
    ```python
    """ All-Reduce 예제 """
    def run(rank, size):
        """ 간단한 집합 통신 """
        group = dist.new_group([0, 1])
        tensor = torch.ones(1)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)
        print('Rank ', rank, ' has data ', tensor[0])
    ```
    
    - 그룹 내의 모든 tensor들의 합이 필요하기 때문에 `dist.ReduceOp.SUM` 을 사용하였다.
    - `dist.ReduceOp.SUM` / `dist.ReduceOp.PRODUCT` / `dist.ReduceOp.MAX` / `dist.ReduceOp.Min`

## 2.5. 분산 학습(Distributed Training)

- SGD의 분산 버전 구현

### 1) Dataset 선언

- 스크립트는 모든 프로세스가 각자의 데이터 배치(batch)에서 각자의 모델의 변화도(gradient)를 계산한 후 평균을 계산한다. 프로세스의 수를 변경해도 유사한 수렴 결과를 보장하기 위해서, 데이터셋을 분할해야 한다.
- Code
    
    ```python
    """ 데이터셋 분할 헬퍼(helper) """
    class Partition(object):
    
        def __init__(self, data, index):
            self.data = data
            self.index = index
    
        def __len__(self):
            return len(self.index)
    
        def __getitem__(self, index):
            data_idx = self.index[index]
            return self.data[data_idx]
    
    class DataPartitioner(object):
    
        def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):
            self.data = data
            self.partitions = []
            rng = Random()
            rng.seed(seed)
            data_len = len(data)
            indexes = [x for x in range(0, data_len)]
            rng.shuffle(indexes)
    
            for frac in sizes:
                part_len = int(frac * data_len)
                self.partitions.append(indexes[0:part_len])
                indexes = indexes[part_len:]
    
        def use(self, partition):
            return Partition(self.data, self.partitions[partition])
    ```
    
    - `DataPartitioner.__init__`
        - 프로세스 개수(rank 개수)만큼 dataset을 partitions list에 나눠준다.
        - partitions list에는 partition으로 나눠진 index 값이 존재한다.
    - `DataPartitioner.use`
        - rank 개수로 나눠진 partitions을 사용한다.
        - 실행되는 rank index를 통해 dataset 일부분을 사용한다.

### 2) DataLoader

- Code
    
    ```python
    """ MNIST 데이터셋 분할 """
    def partition_dataset():
        dataset = datasets.MNIST('./data', train=True, download=True,
                                 transform=transforms.Compose([
                                     transforms.ToTensor(),
                                     transforms.Normalize((0.1307,), (0.3081,))
                                 ]))
        size = dist.get_world_size()
        bsz = 128 / float(size)
        partition_sizes = [1.0 / size for _ in range(size)]
        partition = DataPartitioner(dataset, partition_sizes)
        partition = partition.use(dist.get_rank())
        train_set = torch.utils.data.DataLoader(partition,
                                             batch_size=bsz,
                                             shuffle=True)
        return train_set, bsz
    ```
    
    - dataset에 image paths 를 전달하면 될듯

### 3) Run

- 일반적인 학습 과정과 비슷하다

```python
""" 분산 동기(synchronous) SGD 예제 """
def run(rank, size):
    torch.manual_seed(1234)
    train_set, bsz = partition_dataset()
    model = Net()
    optimizer = optim.SGD(model.parameters(),
                          lr=0.01, momentum=0.5)

    num_batches = ceil(len(train_set.dataset) / float(bsz))
    for epoch in range(10):
        epoch_loss = 0.0
        for data, target in train_set:
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            epoch_loss += loss.item()
            loss.backward()
            average_gradients(model)
            optimizer.step()
        print('Rank ', dist.get_rank(), ', epoch ',
              epoch, ': ', epoch_loss / num_batches)
```

[ average_gradients ]

- 왜 변화도 평균을 계산하는지 잘 모르겠다.

```python
""" 변화도 평균 계산하기 """
def average_gradients(model):
    size = float(dist.get_world_size())
    for param in model.parameters():
        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
        param.grad.data /= size
```

Reference

[Distributed communication package - torch.distributed - PyTorch 1.12 documentation](https://pytorch.org/docs/stable/distributed.html)

[PyTorch로 분산 어플리케이션 개발하기](https://tutorials.pytorch.kr/intermediate/dist_tuto.html)

[A Comprehensive Tutorial to Pytorch DistributedDataParallel](https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)

[Pytorch - DistributedDataParallel (1) - 개요](https://hongl.tistory.com/292?category=927704)

[Pytorch - DistributedDataParallel (2) - 동작 원리](https://hongl.tistory.com/293)

[Distributed data parallel training in Pytorch](https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html)